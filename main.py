import os
import re
import ast
import difflib
from OpenAIAPIConnector import OpenAIAPIConnector
from ResponseParser import ResponseParser
from FileParser import FileParser, FileReader
from RepositoryCloner import RepositoryCloner
from webscraper import RecursiveWebScraper
from pathlib import Path
import subprocess
import shutil  



def normalize_name(filename):
    """Normalize names for fuzzy matching, ignoring extension and common URL artifacts."""
    name = os.path.splitext(filename)[0]
    name = re.sub(r'^(http[s]?_+)?', '', name)  # Remove http_, https_, etc.
    name = name.replace(":", "_")
    name = re.sub(r'[^a-zA-Z0-9_]', '_', name)
    name = re.sub(r'_+', '_', name).strip('_')
    return name.lower()

def restore_url_string(safe_name: str) -> str:
    """Konvertiert einen safe_filename (z.â€¯B. http_localhost_8080_ai.html) zurÃ¼ck zur URL."""
    name = safe_name.rsplit('.', 1)[0]  # Entfernt z.â€¯B. '.html'

    # 1. Protokoll wiederherstellen
    if name.startswith("http_"):
        name = name.replace("http_", "http://", 1)
    elif name.startswith("https_"):
        name = name.replace("https_", "https://", 1)

    # 2. Hostname mit Port finden â€“ z.â€¯B. localhost:8080 oder 127.0.0.1:8000
    # Ersetze das erste '_' nach Host und Port mit ':'
    name = re.sub(r'(?<=http://)([^/_]+)_(\d+)', r'\1:\2', name)
    name = re.sub(r'(?<=https://)([^/_]+)_(\d+)', r'\1:\2', name)

    # 3. Alle weiteren Unterstriche zu Slashes (/) machen
    # z.â€¯B. http://localhost:8080_ai_foo â†’ http://localhost:8080/ai/foo
    name = name.replace("_", "/")

    return name

def combine_requirements_with_scraped_pages(requirements_dir, scraped_dir, output_dir):
    os.makedirs(output_dir, exist_ok=True)

    requirement_files = [f for f in os.listdir(requirements_dir) if f.endswith(".txt")]
    scraped_files = [f for f in os.listdir(scraped_dir) if f.endswith((".txt", ".html"))]

    # Precompute normalized scraped filenames
    scraped_map = {
        normalize_name(f): f for f in scraped_files
    }

    for req_file in requirement_files:
        req_norm = normalize_name(req_file)
        test_url = restore_url_string(req_file)
        best_match = difflib.get_close_matches(req_norm, scraped_map.keys(), n=1, cutoff=0.6)
        if not best_match:
            print(f"âš ï¸ Keine passende scraped Datei fÃ¼r {req_file} gefunden.")
            continue

        matched_scraped_file = scraped_map[best_match[0]]

        req_path = os.path.join(requirements_dir, req_file)
        scraped_path = os.path.join(scraped_dir, matched_scraped_file)
        output_path = os.path.join(output_dir, normalize_name(req_file) + "_combined.txt")

        try:
            with open(scraped_path, "r", encoding="utf-8") as f1, open(req_path, "r", encoding="utf-8") as f2:
                scraped_content = f1.read().strip()
                req_content = f2.read().strip()

            combined = (
                f"##### SCRAPED PAGE #####\n\n"
                f"{scraped_content}\n\n"
                f"##### TEST REQUIREMENTS #####\n\n"
                f"{req_content}"
                f"### TEST URL ###"
                f"{test_url}"
            )

            with open(output_path, "w", encoding="utf-8") as out_file:
                out_file.write(combined)

            print(f"âœ… Kombiniert gespeichert: {output_path}")
        except Exception as e:
            print(f"âŒ Fehler beim Kombinieren von {req_file} mit {matched_scraped_file}: {e}")



def process_image_folder(connector, image_folder: str):
    if not os.path.isdir(image_folder):
        raise ValueError(f"Ordner nicht gefunden: {image_folder}")

    # Create output directory beside image folder
    parent_dir = os.path.dirname(os.path.abspath(image_folder))
    output_dir = os.path.join(parent_dir, "image_requirements")
    os.makedirs(output_dir, exist_ok=True)

    image_files = [
        f for f in os.listdir(image_folder)
        if f.lower().endswith((".png", ".jpg", ".jpeg", ".webp"))
    ]

    if not image_files:
        print("âš ï¸ Keine Bilddateien im Ordner gefunden.")
        return

    for image_file in image_files:
        image_path = os.path.join(image_folder, image_file)
        print(f"ðŸ” Verarbeite Bild: {image_file}")

        try:
            requirements = connector.generate_requirements_from_image(image_path)
            output_path = os.path.join(output_dir, os.path.splitext(image_file)[0] + ".txt")
            with open(output_path, "w", encoding="utf-8") as out_file:
                out_file.write(requirements)
            print(f"âœ… Gespeichert: {output_path}")
        except Exception as e:
            print(f"âŒ Fehler bei {image_file}: {e}")


def setup_directories(repository_name: str):
    base_path = Path(repository_name)
    for subfolder in ["scraped_pages", "tests", "test_results", "code", "images"]:
        (base_path / subfolder).mkdir(parents=True, exist_ok=True)
    return base_path

def is_syntax_valid(filepath):
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            source = f.read()
        compile(source, filepath, "exec")
        return True
    except SyntaxError as e:
        print(f"âŒ Syntaxfehler in {filepath}: {e}")
        return False
    


def contains_test_functions(filepath):
    with open(filepath, "r", encoding="utf-8") as f:
        tree = ast.parse(f.read(), filename=filepath)
    return any(isinstance(node, ast.FunctionDef) and node.name.startswith("test_") for node in tree.body)

def is_collected_by_pytest(filepath):
    result = subprocess.run(
        ["pytest", "--collect-only", str(filepath)],
        capture_output=True,
        text=True
    )
    return "collected 0 items" not in result.stdout and result.returncode == 0

def is_test_runnable(filepath):
    return (
        is_syntax_valid(filepath)
        and contains_test_functions(filepath)
        and is_collected_by_pytest(filepath)
    )


# .\chrome.exe --remote-debugging-port=9222 --user-data-dir="C:\\temp-chrome"

def main(reset=True):
    # Frage den Benutzer nach der URL, die gescraped werden soll
    start_url = input("Bitte gib die URL ein, die gescraped werden soll: ").strip()

    # Initialisiere OpenAI-Connector
    bot = OpenAIAPIConnector(model="gpt-4o-mini")

    if reset:
        bot.reset_state()
        print("ðŸ” Bot-Zustand zurÃ¼ckgesetzt.\n")
        bot = OpenAIAPIConnector(model="gpt-4o-mini")  # Neu instanziieren nach Reset

    #create repository folder
    base_path = setup_directories("run_output")

    # Scrape Website und speichere HTML-Dateien
    scraper = RecursiveWebScraper()
    scraper.start_scraping(start_url=start_url, locationPath=base_path)

    #Get Requirements from image
    process_image_folder(bot, base_path / "images")
    # Kombiniere Requirements mit gescrapten Seiten
    combine_requirements_with_scraped_pages(
        requirements_dir=base_path / "image_requirements",
        scraped_dir=base_path / "scraped_pages",
        output_dir=base_path / "combined_requirements"
    )
    # Lade Requirements Dateien aus Ordner
    requirement_files = [f for f in os.listdir(base_path / "combined_requirements") if f.endswith(".txt")]
    max_files = len(requirement_files)  # Anzahl automatisch auslesen

    for file_data in requirement_files:

        print(f" Frage OpenAI mit {file_data}...")

        try:
            response = bot.ask_with_file(file_data)
        except Exception as e:
            print(f"âŒ Fehler bei Datei {file_data}: {e}")
            continue

        if not response.strip():
            print(f"âš ï¸ Keine Antwort erhalten fÃ¼r Datei {file_data}.")
            continue

        print(f"âœ… Antwort erhalten fÃ¼r Datei {file_data}.")

        # Extrahiere Python-Code aus CodeblÃ¶cken
        code_blocks = re.findall(r"```python(.*?)```", response, re.DOTALL)
        test_code = "\n\n".join(cb.strip() for cb in code_blocks) if code_blocks else response.strip()

        # Speichere generierten Testcode in .py-Datei
        test_output_file = base_path / "tests" / f"test_playwright_{i}.py"
        with open(test_output_file, "w", encoding="utf-8") as f:
            f.write(test_code)

        print(f"ðŸ’¾ Test gespeichert in: {test_output_file}\n")

    print("ðŸŽ¯ Verarbeitung abgeschlossen.\n")

    # ðŸ§ª Testdateien ausfÃ¼hren (ohne Coverage), Output speichern
    test_dir = base_path / "tests"
    test_files = [test_dir / f for f in os.listdir(test_dir) if f.endswith(".py")]

    runnable_tests = [str(f) for f in test_files if is_test_runnable(f)]

    if not runnable_tests:
        print("âš ï¸ Keine lauffÃ¤higen Testdateien gefunden.")
    else:
        print(f"ðŸš€ FÃ¼hre {len(runnable_tests)} Testdateien mit pytest aus...")

        output_log_path = base_path / "test_results" / "pytest_output.log"
        with open(output_log_path, "w", encoding="utf-8") as log_file:
            result = subprocess.run(
                ["pytest"] + runnable_tests,
                cwd=base_path,
                stdout=log_file,
                stderr=subprocess.STDOUT,
                text=True
            )

        if result.returncode == 0:
            print(f"âœ… Alle Tests erfolgreich. Output gespeichert unter: {output_log_path}")
        else:
            print(f"âŒ Einige Tests sind fehlgeschlagen. Siehe Log: {output_log_path}")

    # Optionaler Reset am Ende zur Bereinigung
    if reset:
        bot.reset_state()
        print("ðŸ§¹ Bot-Zustand am Ende zurÃ¼ckgesetzt.")
    #shutil.rmtree(base_path)
    #print("ðŸ—‘ï¸ Ordner 'run_output' wurde gelÃ¶scht.")

if __name__ == "__main__":
    main(reset=True)


