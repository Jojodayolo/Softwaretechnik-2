import os
import re
from OpenAIAPIConnector import OpenAIAPIConnector
from ResponseParser import ResponseParser
from FileParser import FileParser, FileReader
from RepositoryCloner import RepositoryCloner
from webscraper import RecursiveWebScraper

# .\chrome.exe --remote-debugging-port=9222 --user-data-dir="C:\\temp-chrome"

def main(reset=True):
    # TODO
    # repository name aus konsole lesen fÃ¼r Ordnerstruktur
    # Ordnerstruktur erstellen: scraped_pages && tests && test results erzeugen
    # wenn ordnerstruktur -> code so anpassen, dass dateien dementsprechen verarbeitet werden kÃ¶nnen

    # test_playwright_0 etc einmal drÃ¼ber iterieren und nicht lauffÃ¤hige tests lÃ¶schen
    # nach erstellen der tests -> pytest ausfÃ¼hren mit entsprechendem log level -> report erzeugen und in die ordner struktur schreiben
    # nach durchfÃ¼hrung der tests -> ordner struktur mit scraped_pages & tests & test report woanders hinkopieren Ã¼ber pfad
    # nach kopieren der ordnerstruktur ordnerstruktur innerhalb des repos aufrÃ¤umen fÃ¼r den neuen run

    # Frage den Benutzer nach der URL, die gescraped werden soll
    start_url = input("Bitte gib die URL ein, die gescraped werden soll: ").strip()

    # Initialisiere OpenAI-Connector
    bot = OpenAIAPIConnector(model="gpt-4o-mini")

    if reset:
        bot.reset_state()
        print("ğŸ” Bot-Zustand zurÃ¼ckgesetzt.\n")
        bot = OpenAIAPIConnector(model="gpt-4o-mini")  # Neu instanziieren nach Reset

    # Scrape Website und speichere HTML-Dateien
    scraper = RecursiveWebScraper()
    scraper.start_scraping(start_url=start_url)

    # Lade HTML-Dateien aus Ordner
    html_parser = FileParser(folder_path="./scraped_pages")
    html_files = html_parser.read_all_files()
    max_files = len(html_files)  # Anzahl automatisch auslesen

    for i, file_data in enumerate(html_files[:max_files]):
        html_content = file_data['html']
        filename = file_data['filename']
        url = f"/{filename}"  # relative TEST_URL

        # Speichere HTML-Inhalt und zugehÃ¶rige URL in Textdatei
        input_file = f"code{i}.txt"
        with open(input_file, "w", encoding="utf-8") as f:
            f.write(html_content + f"\n\nTEST_URL={url}")

        print(f"[{i}] Frage OpenAI mit {filename}...")

        try:
            response = bot.ask_with_file(input_file)
        except Exception as e:
            print(f"âŒ Fehler bei Datei {filename}: {e}")
            continue

        if not response.strip():
            print(f"âš ï¸ Keine Antwort erhalten fÃ¼r Datei {filename}.")
            continue

        print(f"âœ… Antwort erhalten fÃ¼r Datei {filename}.")

        # Extrahiere Python-Code aus CodeblÃ¶cken
        code_blocks = re.findall(r"```python(.*?)```", response, re.DOTALL)
        test_code = "\n\n".join(cb.strip() for cb in code_blocks) if code_blocks else response.strip()

        # Speichere generierten Testcode in .py-Datei
        test_output_file = f"test_playwright_{i}.py"
        with open(test_output_file, "w", encoding="utf-8") as f:
            f.write(test_code)

        print(f"ğŸ’¾ Test gespeichert in: {test_output_file}\n")

    print("ğŸ¯ Verarbeitung abgeschlossen.\n")

    #TODO
    #AusfÃ¼hren der Tests einbauen

    # Optionaler Reset am Ende zur Bereinigung
    if reset:
        bot.reset_state()
        print("ğŸ§¹ Bot-Zustand am Ende zurÃ¼ckgesetzt.")


if __name__ == "__main__":
    main(reset=True)
